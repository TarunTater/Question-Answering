{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Need nltk installed\n",
    "# No need to run this part\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import string\n",
    "import re\n",
    "import operator\n",
    "import csv\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 960M (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "# Need gensim package installed\n",
    "# No need to run this\n",
    "\n",
    "import gensim\n",
    "import gensim.models as g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need PyLucene installed\n",
    "# No need to run this\n",
    "\n",
    "import lucene\n",
    "\n",
    "from java.io import File \n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer \n",
    "from org.apache.lucene.document import Document, Field \n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig \n",
    "from org.apache.lucene.store import SimpleFSDirectory \n",
    "from org.apache.lucene.util import Version\n",
    "from org.apache.lucene.search import IndexSearcher \n",
    "from org.apache.lucene.index import IndexReader \n",
    "from org.apache.lucene.queryparser.classic import QueryParser \n",
    "\n",
    "from org.apache.lucene import document, store, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Need Keras installed\n",
    "from keras.models import Sequential, Model, Graph\n",
    "from keras.layers import Dense, Activation, Input, Embedding, LSTM, merge, Dropout\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from keras.layers.core import Dense, Flatten, Merge\n",
    "from keras.utils.visualize_util import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert g.doc2vec.FAST_VERSION > -1\n",
    "print cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - To get all documents in one file for input to Doc2Vec\n",
    "file already generated(allText.txt) - no need to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make sure there is no filename already, otherwise overwrites..\n",
    "def rename(dir, pattern):\n",
    "    num = 1\n",
    "    for pathAndFilename in glob.iglob(os.path.join(dir, pattern)):\n",
    "        title, ext = os.path.splitext(os.path.basename(pathAndFilename))\n",
    "        title = str(num)\n",
    "        num = num + 1\n",
    "        os.rename(pathAndFilename, \n",
    "                  os.path.join(dir, title + ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rename(r'/home/tarun/PE/booksCorpus/', r'*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n"
     ]
    }
   ],
   "source": [
    "cachedStopWords = stopwords.words(\"english\")\n",
    "print len(cachedStopWords)\n",
    "updateCachedStopWords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'yo', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'or', 'until',   'once', 'here', 'there', 'other', 's', 't', 'can', 'will', 'just', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dont forget to remove the allText.txt file everytime you run this code.\n",
    "dirName = \"/home/tarun/PE/newCorpus60/\"\n",
    "uniqueWords = {}\n",
    "allText = \"\"\n",
    "l = os.listdir(dirName)\n",
    "for i in range(len(l)):\n",
    "    l[i] = int(l[i][:-4])\n",
    "\n",
    "l = sorted(l)\n",
    "for i in range(len(l)):\n",
    "    l[i] = str(l[i]) + \".txt\"\n",
    "\n",
    "for fname in l:\n",
    "    with open(os.path.join(dirName, fname), 'r') as myFile:\n",
    "        fileText = myFile.read().replace('\\n', ' ') #each file's text - a paragraph like structure for us\n",
    "        for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "            fileText = fileText.replace(char, '')\n",
    "        fileText = fileText.replace(\"displaystyle\", '')\n",
    "        fileText = re.sub(\"\"\"displaystyle\"\"\", \"\", fileText, re.I|re.S)\n",
    "        fileText = re.sub(\"\"\"[^0-9A-Za-z]\"\"\", ' ', fileText)\n",
    "\n",
    "        fileText = re.sub(\"\"\"\\s+\"\"\", \" \", fileText)\n",
    "        fileText = re.sub(\"\"\"\\t+\"\"\", \" \", fileText)\n",
    "        fileText = ' '.join([word for word in fileText.split() if word not in updateCachedStopWords])\n",
    "        #fileText = ' '.join([word for word in fileText.split()])\n",
    "        allText = allText + fileText\n",
    "        allText = allText + \"\\n\"\n",
    "    with open(os.path.join(dirName, fname), 'w') as myFile:\n",
    "        myFile.write(fileText)\n",
    "with open(os.path.join(\"/home/tarun/PE/\", \"newCorpus60.txt\"), 'w') as myFile:\n",
    "    myFile.write(allText)\n",
    "#WOSP means without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(\"/home/tarun/PE/\", \"allTextBooks.txt\"), 'r') as myFile:\n",
    "    allTokens = []\n",
    "    fileText = myFile.read()\n",
    "    paras = fileText.split(\"\\n\")\n",
    "    for eachPara in paras:\n",
    "        #print eachPara\n",
    "        tokens1 = nltk.word_tokenize(eachPara)\n",
    "        if(len(tokens1) > 0):\n",
    "            allTokens.append(tokens1)\n",
    "with open(os.path.join(\"/home/tarun/PE/\", \"allTextTokensBooks.txt\"), 'wb') as myFile:\n",
    "    pickle.dump(allTokens, myFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(\"/home/tarun/PE/\", \"allTextTokensBooks.txt\"), 'rb') as myFile:\n",
    "    allTokens = pickle.load(myFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of cores (to be given as paramter in the Doc2Vec training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenizeSentence(fileText):\n",
    "    #fileText = \"maddy is mad. tarun is super mad!\"\n",
    "    fileText = fileText.replace('\\n', ' ') #each file's text - a paragraph like structure for us\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "        fileText = fileText.replace(char, '')\n",
    "    fileText = fileText.replace(\"displaystyle\", '')\n",
    "    fileText = re.sub(\"\"\"displaystyle\"\"\", \"\", fileText, re.I|re.S)\n",
    "    fileText = re.sub(\"\"\"[^0-9A-Za-z]\"\"\", ' ', fileText)\n",
    "\n",
    "    fileText = re.sub(\"\"\"\\s+\"\"\", \" \", fileText)\n",
    "    fileText = re.sub(\"\"\"\\t+\"\"\", \" \", fileText)\n",
    "    tokens1 = nltk.word_tokenize(fileText)\n",
    "    return tokens1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the Doc2Vec Model\n",
    "Model already trained with the below mentioned parameters. Two available models - model2 and model3!\n",
    "Using model2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to run this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-11-28 22:47:31,534 : INFO : collecting all words and their counts\n",
      "2016-11-28 22:47:31,535 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2016-11-28 22:47:31,756 : INFO : PROGRESS: at example #10000, processed 556169 words (2517764/s), 33599 word types, 10000 tags\n",
      "2016-11-28 22:47:31,958 : INFO : PROGRESS: at example #20000, processed 1111024 words (2757427/s), 48486 word types, 20000 tags\n",
      "2016-11-28 22:47:32,160 : INFO : PROGRESS: at example #30000, processed 1667660 words (2756484/s), 60196 word types, 30000 tags\n",
      "2016-11-28 22:47:32,358 : INFO : PROGRESS: at example #40000, processed 2224022 words (2815394/s), 69479 word types, 40000 tags\n",
      "2016-11-28 22:47:32,617 : INFO : PROGRESS: at example #50000, processed 2777446 words (2147585/s), 77991 word types, 50000 tags\n",
      "2016-11-28 22:47:32,941 : INFO : PROGRESS: at example #60000, processed 3332188 words (1715698/s), 84377 word types, 60000 tags\n",
      "2016-11-28 22:47:33,157 : INFO : PROGRESS: at example #70000, processed 3887651 words (2581388/s), 91930 word types, 70000 tags\n",
      "2016-11-28 22:47:33,468 : INFO : PROGRESS: at example #80000, processed 4443139 words (1788775/s), 97935 word types, 80000 tags\n",
      "2016-11-28 22:47:33,693 : INFO : PROGRESS: at example #90000, processed 4997442 words (2471137/s), 104161 word types, 90000 tags\n",
      "2016-11-28 22:47:33,900 : INFO : PROGRESS: at example #100000, processed 5550777 words (2678678/s), 110597 word types, 100000 tags\n",
      "2016-11-28 22:47:34,110 : INFO : PROGRESS: at example #110000, processed 6107490 words (2654796/s), 115498 word types, 110000 tags\n",
      "2016-11-28 22:47:34,300 : INFO : PROGRESS: at example #120000, processed 6659061 words (2916435/s), 115820 word types, 120000 tags\n",
      "2016-11-28 22:47:34,499 : INFO : PROGRESS: at example #130000, processed 7218153 words (2821988/s), 116429 word types, 130000 tags\n",
      "2016-11-28 22:47:34,702 : INFO : PROGRESS: at example #140000, processed 7763919 words (2689736/s), 117816 word types, 140000 tags\n",
      "2016-11-28 22:47:34,912 : INFO : PROGRESS: at example #150000, processed 8320121 words (2657983/s), 122157 word types, 150000 tags\n",
      "2016-11-28 22:47:35,069 : INFO : collected 125709 word types and 157136 unique tags from a corpus of 157136 examples and 8717112 words\n",
      "2016-11-28 22:47:35,544 : INFO : min_count=1 retains 125709 unique words (drops 0)\n",
      "2016-11-28 22:47:35,544 : INFO : min_count leaves 8717112 word corpus (100% of original 8717112)\n",
      "2016-11-28 22:47:35,816 : INFO : deleting the raw counts dictionary of 125709 items\n",
      "2016-11-28 22:47:35,821 : INFO : sample=1e-05 downsamples 4636 most-common words\n",
      "2016-11-28 22:47:35,822 : INFO : downsampling leaves estimated 3241362 word corpus (37.2% of prior 8717112)\n",
      "2016-11-28 22:47:35,823 : INFO : estimated required memory for 125709 words and 100 dimensions: 829679300 bytes\n",
      "2016-11-28 22:47:36,191 : INFO : using concatenative 1300-dimensional layer1\n",
      "2016-11-28 22:47:36,191 : INFO : resetting layer weights\n",
      "2016-11-28 22:47:38,627 : INFO : training model with 8 workers on 125710 vocabulary and 1300 features, using sg=0 hs=0 sample=1e-05 negative=5\n",
      "2016-11-28 22:47:38,628 : INFO : expecting 157136 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-11-28 22:47:39,746 : INFO : PROGRESS: at 0.57% examples, 86026 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:47:40,760 : INFO : PROGRESS: at 1.32% examples, 107303 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:47:41,763 : INFO : PROGRESS: at 2.10% examples, 116479 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:47:42,789 : INFO : PROGRESS: at 2.86% examples, 119254 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:47:43,803 : INFO : PROGRESS: at 3.65% examples, 122425 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:47:44,814 : INFO : PROGRESS: at 4.41% examples, 122602 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:47:45,814 : INFO : PROGRESS: at 5.18% examples, 123759 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:47:46,838 : INFO : PROGRESS: at 5.96% examples, 124720 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:47:47,841 : INFO : PROGRESS: at 6.72% examples, 125096 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:47:48,858 : INFO : PROGRESS: at 7.48% examples, 125424 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:47:49,892 : INFO : PROGRESS: at 8.25% examples, 125896 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:47:50,897 : INFO : PROGRESS: at 9.00% examples, 126421 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:47:51,901 : INFO : PROGRESS: at 9.75% examples, 126998 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:47:52,915 : INFO : PROGRESS: at 10.56% examples, 127383 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:47:53,927 : INFO : PROGRESS: at 11.29% examples, 127114 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:47:54,939 : INFO : PROGRESS: at 12.02% examples, 127122 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:47:55,950 : INFO : PROGRESS: at 12.85% examples, 127596 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:47:56,973 : INFO : PROGRESS: at 13.58% examples, 127471 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:47:57,977 : INFO : PROGRESS: at 14.38% examples, 127897 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:47:59,003 : INFO : PROGRESS: at 15.23% examples, 127940 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:48:00,013 : INFO : PROGRESS: at 16.05% examples, 128059 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:48:01,029 : INFO : PROGRESS: at 16.89% examples, 128056 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:48:02,043 : INFO : PROGRESS: at 17.69% examples, 127885 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:03,050 : INFO : PROGRESS: at 18.45% examples, 127914 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:04,059 : INFO : PROGRESS: at 19.20% examples, 127941 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:48:05,079 : INFO : PROGRESS: at 19.93% examples, 127981 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:06,094 : INFO : PROGRESS: at 20.73% examples, 128195 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:07,152 : INFO : PROGRESS: at 21.48% examples, 128090 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:48:08,219 : INFO : PROGRESS: at 22.26% examples, 128041 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:48:09,222 : INFO : PROGRESS: at 22.99% examples, 128046 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:10,235 : INFO : PROGRESS: at 23.76% examples, 128162 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:48:11,253 : INFO : PROGRESS: at 24.56% examples, 128132 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:12,268 : INFO : PROGRESS: at 25.32% examples, 128158 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:13,274 : INFO : PROGRESS: at 26.05% examples, 128073 words/s, in_qsize 14, out_qsize 1\n",
      "2016-11-28 22:48:14,280 : INFO : PROGRESS: at 26.83% examples, 128174 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:15,280 : INFO : PROGRESS: at 27.56% examples, 128183 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:48:16,304 : INFO : PROGRESS: at 28.32% examples, 128110 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:17,332 : INFO : PROGRESS: at 29.07% examples, 128178 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:48:18,336 : INFO : PROGRESS: at 29.78% examples, 128126 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:19,353 : INFO : PROGRESS: at 30.56% examples, 128114 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:20,355 : INFO : PROGRESS: at 31.29% examples, 128037 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:21,369 : INFO : PROGRESS: at 32.05% examples, 128092 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:22,397 : INFO : PROGRESS: at 32.87% examples, 128206 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:23,417 : INFO : PROGRESS: at 33.60% examples, 128169 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:24,434 : INFO : PROGRESS: at 34.36% examples, 128110 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:25,480 : INFO : PROGRESS: at 35.25% examples, 128233 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:26,491 : INFO : PROGRESS: at 36.07% examples, 128261 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:27,520 : INFO : PROGRESS: at 36.92% examples, 128229 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:28,593 : INFO : PROGRESS: at 37.67% examples, 127843 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:29,609 : INFO : PROGRESS: at 38.42% examples, 127804 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:30,634 : INFO : PROGRESS: at 39.17% examples, 127819 words/s, in_qsize 14, out_qsize 1\n",
      "2016-11-28 22:48:31,638 : INFO : PROGRESS: at 39.90% examples, 127891 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:32,652 : INFO : PROGRESS: at 40.68% examples, 127911 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:48:33,674 : INFO : PROGRESS: at 41.41% examples, 127883 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:34,693 : INFO : PROGRESS: at 42.19% examples, 128000 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:35,729 : INFO : PROGRESS: at 42.94% examples, 127983 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:36,734 : INFO : PROGRESS: at 43.72% examples, 128047 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:37,736 : INFO : PROGRESS: at 44.47% examples, 127954 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:38,789 : INFO : PROGRESS: at 45.22% examples, 127868 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:39,835 : INFO : PROGRESS: at 46.00% examples, 127897 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:40,861 : INFO : PROGRESS: at 46.81% examples, 127961 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:41,887 : INFO : PROGRESS: at 47.52% examples, 127840 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:42,932 : INFO : PROGRESS: at 48.27% examples, 127788 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:43,977 : INFO : PROGRESS: at 49.00% examples, 127733 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:45,026 : INFO : PROGRESS: at 49.73% examples, 127680 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:46,032 : INFO : PROGRESS: at 50.51% examples, 127706 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:47,034 : INFO : PROGRESS: at 51.24% examples, 127663 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:48:48,046 : INFO : PROGRESS: at 51.98% examples, 127668 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:49,070 : INFO : PROGRESS: at 52.82% examples, 127799 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:50,078 : INFO : PROGRESS: at 53.55% examples, 127790 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:48:51,163 : INFO : PROGRESS: at 54.33% examples, 127703 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:52,204 : INFO : PROGRESS: at 55.23% examples, 127804 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:48:53,226 : INFO : PROGRESS: at 56.02% examples, 127767 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:48:54,230 : INFO : PROGRESS: at 56.87% examples, 127790 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:48:55,289 : INFO : PROGRESS: at 57.64% examples, 127606 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:56,335 : INFO : PROGRESS: at 58.37% examples, 127486 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:57,336 : INFO : PROGRESS: at 59.13% examples, 127543 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:58,343 : INFO : PROGRESS: at 59.84% examples, 127545 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:48:59,345 : INFO : PROGRESS: at 60.59% examples, 127524 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:00,347 : INFO : PROGRESS: at 61.32% examples, 127530 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:01,376 : INFO : PROGRESS: at 62.10% examples, 127595 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:02,384 : INFO : PROGRESS: at 62.83% examples, 127573 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:03,388 : INFO : PROGRESS: at 63.62% examples, 127680 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:04,393 : INFO : PROGRESS: at 64.38% examples, 127657 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:05,422 : INFO : PROGRESS: at 65.18% examples, 127698 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:06,456 : INFO : PROGRESS: at 65.96% examples, 127729 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:07,475 : INFO : PROGRESS: at 66.74% examples, 127759 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:08,493 : INFO : PROGRESS: at 67.49% examples, 127757 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:09,521 : INFO : PROGRESS: at 68.25% examples, 127759 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:10,581 : INFO : PROGRESS: at 69.00% examples, 127733 words/s, in_qsize 14, out_qsize 1\n",
      "2016-11-28 22:49:11,581 : INFO : PROGRESS: at 69.75% examples, 127810 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:12,590 : INFO : PROGRESS: at 70.53% examples, 127817 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:13,595 : INFO : PROGRESS: at 71.29% examples, 127831 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:14,600 : INFO : PROGRESS: at 72.04% examples, 127875 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:15,655 : INFO : PROGRESS: at 72.87% examples, 127891 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:16,668 : INFO : PROGRESS: at 73.60% examples, 127881 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:17,672 : INFO : PROGRESS: at 74.38% examples, 127918 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:18,700 : INFO : PROGRESS: at 75.25% examples, 127959 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:19,759 : INFO : PROGRESS: at 76.14% examples, 128018 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:20,773 : INFO : PROGRESS: at 76.99% examples, 128026 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:21,775 : INFO : PROGRESS: at 77.80% examples, 128062 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:22,779 : INFO : PROGRESS: at 78.56% examples, 128062 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:23,792 : INFO : PROGRESS: at 79.31% examples, 128082 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:24,802 : INFO : PROGRESS: at 80.04% examples, 128100 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:25,814 : INFO : PROGRESS: at 80.80% examples, 128085 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:26,819 : INFO : PROGRESS: at 81.55% examples, 128123 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:27,853 : INFO : PROGRESS: at 82.32% examples, 128145 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:28,873 : INFO : PROGRESS: at 83.10% examples, 128198 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:29,881 : INFO : PROGRESS: at 83.88% examples, 128237 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:30,893 : INFO : PROGRESS: at 84.67% examples, 128238 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:31,939 : INFO : PROGRESS: at 85.43% examples, 128208 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:32,962 : INFO : PROGRESS: at 86.23% examples, 128268 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:33,987 : INFO : PROGRESS: at 86.99% examples, 128234 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:35,001 : INFO : PROGRESS: at 87.74% examples, 128257 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:36,004 : INFO : PROGRESS: at 88.50% examples, 128296 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:37,014 : INFO : PROGRESS: at 89.25% examples, 128316 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:38,017 : INFO : PROGRESS: at 89.98% examples, 128316 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:39,052 : INFO : PROGRESS: at 90.76% examples, 128294 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:40,071 : INFO : PROGRESS: at 91.56% examples, 128358 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:41,080 : INFO : PROGRESS: at 92.32% examples, 128330 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:42,153 : INFO : PROGRESS: at 93.12% examples, 128321 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:43,187 : INFO : PROGRESS: at 93.90% examples, 128362 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:44,205 : INFO : PROGRESS: at 94.70% examples, 128385 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:45,220 : INFO : PROGRESS: at 95.59% examples, 128414 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:46,228 : INFO : PROGRESS: at 96.39% examples, 128433 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:47,243 : INFO : PROGRESS: at 97.28% examples, 128423 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:48,264 : INFO : PROGRESS: at 98.06% examples, 128438 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:49,278 : INFO : PROGRESS: at 98.81% examples, 128450 words/s, in_qsize 16, out_qsize 0\n",
      "2016-11-28 22:49:50,287 : INFO : PROGRESS: at 99.56% examples, 128473 words/s, in_qsize 15, out_qsize 0\n",
      "2016-11-28 22:49:50,653 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2016-11-28 22:49:50,672 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2016-11-28 22:49:50,685 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2016-11-28 22:49:50,733 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2016-11-28 22:49:50,734 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-11-28 22:49:50,752 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-11-28 22:49:50,762 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-11-28 22:49:50,764 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-11-28 22:49:50,765 : INFO : training on 43585560 raw words (16992729 effective words) took 132.1s, 128602 effective words/s\n",
      "2016-11-28 22:49:50,842 : INFO : saving Doc2Vec object under /home/tarun/PE/doc2vec/model3_100_newCorpus60_1min_6window_100trainEpoch.bin, separately None\n",
      "2016-11-28 22:49:50,843 : INFO : storing numpy array 'doctag_syn0' to /home/tarun/PE/doc2vec/model3_100_newCorpus60_1min_6window_100trainEpoch.bin.docvecs.doctag_syn0.npy\n",
      "2016-11-28 22:49:50,884 : INFO : storing numpy array 'syn1neg' to /home/tarun/PE/doc2vec/model3_100_newCorpus60_1min_6window_100trainEpoch.bin.syn1neg.npy\n",
      "2016-11-28 22:49:52,070 : INFO : not storing attribute syn0norm\n",
      "2016-11-28 22:49:52,071 : INFO : storing numpy array 'syn0' to /home/tarun/PE/doc2vec/model3_100_newCorpus60_1min_6window_100trainEpoch.bin.syn0.npy\n",
      "2016-11-28 22:49:52,446 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142.16680789\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "#doc2vec parameters\n",
    "vector_size = 100\n",
    "window_size = 6\n",
    "min_count = 1\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100\n",
    "dm = 1 #0 = dbow; 1 = dmpv\n",
    "worker_count = cores #number of parallel processes - number of cores\n",
    "\n",
    "#input corpus\n",
    "train_corpus = \"/home/tarun/PE/newCorpus60.txt\"\n",
    "\n",
    "#output model\n",
    "saved_path = \"/home/tarun/PE/doc2vec/model3_100_newCorpus60_1min_6window_100trainEpoch.bin\"\n",
    "\n",
    "#enable logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#train doc2vec model\n",
    "docs = g.doc2vec.TaggedLineDocument(train_corpus)\n",
    "\n",
    "#model2 - without dbow_words\n",
    "#model3 - with dbow_words\n",
    "model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1) #model3\n",
    "#model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dm_concat=1) #model2\n",
    "\n",
    "#save model\n",
    "model.save(saved_path)\n",
    "toc = time.time()\n",
    "print toc - tic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Retrival system using PyLucene\n",
    "Files already generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PATHS \n",
    "luceneIndexPath = '/home/tarun/PE/lucene/luceneIndexDirectoryNewCorpus60/'\n",
    "corpus = '/home/tarun/PE/newCorpus60/'\n",
    "trainingFilePath = '/home/tarun/PE/Dataset/training_set.tsv'\n",
    "\n",
    "#inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "    \n",
    "lucene.initVM()\n",
    "\n",
    "# ANALYZER\n",
    "analyzer = StandardAnalyzer(util.Version.LUCENE_CURRENT) \n",
    "\n",
    "# DIRECTORY\n",
    "directory = SimpleFSDirectory(File(luceneIndexPath))\n",
    "\n",
    "\n",
    "#dont forget to remove the luceneIndexDirectory file everytime you run this code.\n",
    "\n",
    "# INDEX WRITER\n",
    "writerConfig = IndexWriterConfig(util.Version.LUCENE_CURRENT, analyzer) \n",
    "writer = IndexWriter(directory, writerConfig)\n",
    "\n",
    "l = os.listdir(corpus)\n",
    "for i in range(len(l)):\n",
    "    l[i] = int(l[i][:-4])\n",
    "\n",
    "l = sorted(l)\n",
    "for i in range(len(l)):\n",
    "    l[i] = str(l[i]) + \".txt\"\n",
    "\n",
    "#print writer.numDocs()\n",
    "# INDEXING ALL DOCUMENTS/ARTICLES IN THE CORPUS\n",
    "for fileName in l:\n",
    "    #print fileName\n",
    "    document = Document()\n",
    "    article = os.path.join(corpus, fileName)\n",
    "    content = open(article, 'r').read()\n",
    "    document.add(Field(\"text\", content, Field.Store.YES, Field.Index.ANALYZED))\n",
    "    writer.addDocument(document)\n",
    "#print writer.numDocs()\n",
    "writer.close()\n",
    "\n",
    "# INDEX READER\n",
    "reader = IndexReader.open(directory)\n",
    "searcher = IndexSearcher(reader)\n",
    "\n",
    "# QUERYING FOR A QUESTION\n",
    "queryParser = QueryParser(util.Version.LUCENE_CURRENT, \"text\", analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training File\n",
    "## Retrieving the top n (=3) documents relevant to the question and getting question, paragraph and option vectors to create the input vector\n",
    "Training file with input vectors and output already generated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to run this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "trainingFilePath = '/home/tarun/PE/Dataset/training_set.tsv'\n",
    "\n",
    "with open(trainingFilePath) as trainData:\n",
    "    reader = csv.reader(trainData, delimiter=\"\\t\")\n",
    "    header = 0\n",
    "\n",
    "    storeQidDocs = {}\n",
    "    \n",
    "    for row in reader:\n",
    "        if (header == 0):\n",
    "            header = 1\n",
    "            continue\n",
    "        else:\n",
    "            question = row[1]\n",
    "            docNums = []\n",
    "            # Retrieving the top n (=3) documents relevant to the question\n",
    "            query = queryParser.parse(queryParser.escape(question))\n",
    "            numPages = 3\n",
    "            hits = searcher.search(query, numPages)\n",
    "            for hit in hits.scoreDocs:\n",
    "                doc_id = hit.doc\n",
    "                docNums.append(doc_id)\n",
    "            storeQidDocs[question] = docNums\n",
    "            \n",
    "# Store data (serialize)\n",
    "with open('qidPagesTrain.pickle', 'wb') as handle:\n",
    "    pickle.dump(storeQidDocs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            \n",
    "toc = time.time()\n",
    "print toc - tic\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No need to run this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tic = time.time()\n",
    "with open('/home/tarun/PE/Dataset/final_test_set.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "for row in test:\n",
    "    question = row[1]\n",
    "    docNums = []\n",
    "    # Retrieving the top n (=3) documents relevant to the question\n",
    "    query = queryParser.parse(queryParser.escape(question))\n",
    "    numPages = 3\n",
    "    hits = searcher.search(query, numPages)\n",
    "    for hit in hits.scoreDocs:\n",
    "        doc_id = hit.doc\n",
    "        docNums.append(doc_id)\n",
    "    storeQidDocs[question] = docNums\n",
    "            \n",
    "# Store data (serialize)\n",
    "with open('qidPagesTest.pickle', 'wb') as handle:\n",
    "    pickle.dump(storeQidDocs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            \n",
    "toc = time.time()\n",
    "print toc - tic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(\"/home/tarun/PE/corpus\", \"2.txt\"), 'r') as myFile:\n",
    "    fileText = tokenizeSentence(myFile.read())\n",
    "    \n",
    "check = model.infer_vector(fileText, alpha=start_alpha, steps=infer_epoch)\n",
    "check20 = model.docvecs[0]\n",
    "check21 = model.docvecs[1]\n",
    "check22 = model.docvecs[2]\n",
    "check23 = model.docvecs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(\"/home/tarun/PE/corpus\", \"4.txt\"), 'r') as myFile:\n",
    "    fileText = myFile.read()\n",
    "    \n",
    "check2 = model.infer_vector(fileText, alpha=start_alpha, steps=infer_epoch)\n",
    "check23 = model.docvecs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print check, \"\\n\\n\\n\"\n",
    "#print check1\n",
    "result = 1 - spatial.distance.cosine(check2, check23)\n",
    "print result\n",
    "#print check2\n",
    "#print check23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-10-31 17:43:57,415 : INFO : loading Doc2Vec object from /home/tarun/PE/doc2vec/model3_100_NewCorpusWOSP_1min_2hit_4window.bin\n",
      "2016-10-31 17:43:57,885 : INFO : loading docvecs recursively from /home/tarun/PE/doc2vec/model3_100_NewCorpusWOSP_1min_2hit_4window.bin.docvecs.* with mmap=None\n",
      "2016-10-31 17:43:57,886 : INFO : loading syn1neg from /home/tarun/PE/doc2vec/model3_100_NewCorpusWOSP_1min_2hit_4window.bin.syn1neg.npy with mmap=None\n",
      "2016-10-31 17:43:58,021 : INFO : loading syn0 from /home/tarun/PE/doc2vec/model3_100_NewCorpusWOSP_1min_2hit_4window.bin.syn0.npy with mmap=None\n",
      "2016-10-31 17:43:58,036 : INFO : setting ignored attribute syn0norm to None\n",
      "2016-10-31 17:43:58,037 : INFO : setting ignored attribute cum_table to None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 36.4908339977\n",
      "1000 73.8320231438\n",
      "1500 111.183315039\n",
      "2000 149.428488016\n",
      "2500 184.372644186\n",
      "184.381067991\n"
     ]
    }
   ],
   "source": [
    "modelPath=\"/home/tarun/PE/doc2vec/model3_100_NewCorpusWOSP_1min_2hit_4window.bin\"\n",
    "model = g.Doc2Vec.load(modelPath)\n",
    "#inference hyper-parameters\n",
    "tic = time.time()\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "trainingFilePath = '/home/tarun/PE/Dataset/training_set.tsv'\n",
    "\n",
    "answers = ['A','B','C','D']\n",
    "\n",
    "with open(trainingFilePath) as f:\n",
    "    reader = csv.reader(f,delimiter = \",\")\n",
    "    data = list(reader)\n",
    "    numQuestions = len(data) - 1\n",
    "\n",
    "with open(trainingFilePath) as trainData:\n",
    "    reader = csv.reader(trainData, delimiter=\"\\t\")\n",
    "    header = 0\n",
    "    storeInputVecInFile = np.zeros([numQuestions,700])\n",
    "    storeOutputVecInFile = np.zeros([numQuestions,4])\n",
    "    inputNum = 0\n",
    "    for row in reader:\n",
    "        if (header == 0):\n",
    "            header = 1\n",
    "            continue\n",
    "        else:\n",
    "            question = row[1]\n",
    "            # Retrieving the top n (=3) documents relevant to the question\n",
    "            query = queryParser.parse(queryParser.escape(question))\n",
    "            question = tokenizeSentence(question)\n",
    "            questionVec = model.infer_vector(question, alpha=start_alpha, steps=infer_epoch)\n",
    "            numPages = 2\n",
    "            hits = searcher.search(query, numPages)\n",
    "            docVec = np.zeros(200)\n",
    "            start=0\n",
    "            output = []\n",
    "            for hit in hits.scoreDocs:\n",
    "                doc_id = hit.doc\n",
    "                docV = model.docvecs[doc_id]\n",
    "                docVec[start:start+100] = docV\n",
    "                start = start+100\n",
    "                \n",
    "            optionList = [row[3], row[4], row[5], row[6]]\n",
    "            inputVec = np.concatenate([docVec, questionVec])\n",
    "            output = []\n",
    "            for option in optionList:\n",
    "                optionVec = model.infer_vector(tokenizeSentence(option), alpha=start_alpha, steps=infer_epoch)\n",
    "                if(answers[optionList.index(option)] == row[2]):\n",
    "                    output.append(1)\n",
    "                else:\n",
    "                    output.append(0)\n",
    "                    \n",
    "                #inputVec = merge([docVec, questionVec, optionVec], mode='concat')\n",
    "                inputVec = np.concatenate([inputVec, optionVec])\n",
    "            storeInputVecInFile[inputNum] = inputVec\n",
    "            storeOutputVecInFile[inputNum] = output\n",
    "            inputNum = inputNum + 1\n",
    "            if(inputNum % 500 == 0):\n",
    "                print inputNum, (time.time() - tic)\n",
    "# Save the input vectors and output into a file\n",
    "np.save('inputTokensModel3_100_NewCorpusWOSP_1min_2hit_4window.bin', storeInputVecInFile)\n",
    "np.save('outputTokensModel3_100_NewCorpusWOSP_1min_2hit_4window.bin', storeOutputVecInFile)\n",
    "toc = time.time()\n",
    "print toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Trying to make copies of the options and seeing if questions can be answered looking at the paragraph word by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7465 doc=7465 score=0.8155148 shardIndex=-1\n",
      "Newton s Laws Introduction The Big Idea Applied forces may cause objects to accelerate All forces come in pairs because they arise in the interaction of two objects you can t push without being pushed back The more force applied the greater the acceleration that is produced Objects with high masses are difficult to accelerate without a large force In the absence of applied forces objects move in a straight line at a constant speed or remain at rest In formal language Newton s First Law Every body continues in its state of rest or of uniform motion in a right straight line unless it is compelled to change that state by forces impressed upon it Newton s Second Law The change of motion is proportional to the motive force impressed and is made in the direction of the right straight line in which that force is impressed Newton s Third Law To every action there is always opposed an equal reaction or the mutual actions of two bodies upon each other are always equal and directed to contrary parts Taken from the Principia in modern English Isaac Newton University of California Press 1934 Understanding motion comes in two stages The first stage you ve already seen you can figure out where something will go and how fast it will get there if you know its acceleration The second stage is much more interesting where did the acceleration come from How can you predict the amount of acceleration Mastering both stages is the key to understanding motion Newton s First Law Describe Newton s first law The First Law is about inertia objects at rest stay at rest unless acted upon and objects in motion continue that motion in a straight line unless acted upon Prior to Newton and Galileo the prevailing view on motion was still Aristotle s According to his theory the natural state of things is at rest force is required to keep something moving at a constant rate This made sense to people throughout history because on earth friction and air resistance slow moving objects When there is no air resistance or other sources of friction a situation approximated in space Newton s first law is much more evident The amount of inertia an object has is simply related to the mass of the object Mass and Weight are two different things Mass typically in units of kg or grams is basically a measure of what comprises an object Weight is\n",
      "14648 doc=14648 score=0.7886188 shardIndex=-1\n",
      "matter Particles of solids have the least kinetic energy and particles of gases have the most Explore More Watch the video below https wwwyoutubecom watchv KCL8zqjXbME and then answer the questions that follow Click on the image above for more content Questions Describe the motion of particles in ice liquid water and water vapor Apply the kinetic theory of matter to explain the differences in your answer to question 1 Review State the kinetic theory of matter Explain the relationship between kinetic energy and state of matter Changes of State Define change of state Identify processes that cause changes of state Explain the role of energy in changes of state Both of these photos show the famous Golden Gate Bridge near San Francisco California The pictures were taken from about the same point of view but they look very different In the picture on the left the deck of the bridge is almost completely hidden by a thick layer of fog In the picture on the right the fog has disappeared and the deck of the bridge as well as the water below it is clearly visible Fog consists of tiny droplets of liquid water The fog in the picture is like a cloud at ground level Where did the fog come from and where did it go What Are Changes of State The water droplets of fog form from water vapor in the air Fog disappears when the water droplets change back to water vapor These changes are examples of changes of state A change of state occurs whenever matter changes from one state to another Common states of matter on Earth are solid liquid and gas Matter may change back and forth between any two of these states Changes of state are physical changes in matter They are reversible changes that do not change matter s chemical makeup or chemical properties For example when fog changes to water vapor it is still water and can change back to liquid water again Processes that Cause Changes of State Several processes are involved in common changes of state They include melting freezing sublimation deposition condensation and evaporation The Figure below shows how matter changes in each of these processes Figure 2836 Q Which two processes result in matter changing to the solid state A The processes are deposition in which matter changes from a gas to a solid and freezing in which matter changes from a liquid to a solid The Role of Energy in\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the top n (=3) documents relevant to the question\n",
    "question = \"What is the cause of change in motion or change in the state of motion?\"\n",
    "query = queryParser.parse(queryParser.escape(question))\n",
    "question = tokenizeSentence(question)\n",
    "questionVec = model.infer_vector(question, alpha=start_alpha, steps=infer_epoch)\n",
    "numPages = 1\n",
    "hits = searcher.search(query, numPages)\n",
    "#docVec = np.zeros(200)\n",
    "start=0\n",
    "docText = \"\"\n",
    "output = []\n",
    "for hit in hits.scoreDocs:\n",
    "    doc_id = hit.doc\n",
    "    print doc_id, hit.toString()\n",
    "    docT = searcher.doc(hit.doc)\n",
    "    docText = docT.get(\"text\").encode(\"utf-8\")\n",
    "\n",
    "listOfWords = docText.split(\" \")\n",
    "numWords = len(listOfWords)\n",
    "vector_size = 100\n",
    "num = 0\n",
    "docVec = np.zeros([(numWords),vector_size])\n",
    "for eachWord in listOfWords:\n",
    "    docV = model.docvecs[doc_id]\n",
    "    docVec[num] = docV\n",
    "    num = num + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trial\n",
    "modelPath=\"/home/tarun/PE/doc2vec/model3_100_NewCorpus_1min_4window.bin\"\n",
    "model = g.Doc2Vec.load(modelPath)\n",
    "#inference hyper-parameters\n",
    "tic = time.time()\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "trainingFilePath = '/home/tarun/PE/Dataset/training_set.tsv'\n",
    "\n",
    "answers = ['A','B','C','D']\n",
    "\n",
    "with open(trainingFilePath) as f:\n",
    "    reader = csv.reader(f,delimiter = \",\")\n",
    "    data = list(reader)\n",
    "    numQuestions = len(data) - 1\n",
    "\n",
    "with open(trainingFilePath) as trainData:\n",
    "    reader = csv.reader(trainData, delimiter=\"\\t\")\n",
    "    header = 0\n",
    "    storeInputVecInFile = np.zeros([numQuestions,700]) #question, docs, options\n",
    "    storeOutputVecInFile = np.zeros([numQuestions,4])\n",
    "    inputNum = 0\n",
    "    for row in reader:\n",
    "        if (header == 0):\n",
    "            header = 1\n",
    "            continue\n",
    "        else:\n",
    "            question = row[1]\n",
    "            query = queryParser.parse(queryParser.escape(question))\n",
    "            question = tokenizeSentence(question)\n",
    "            questionVec = model.infer_vector(question, alpha=start_alpha, steps=infer_epoch)\n",
    "            numPages = 1\n",
    "            hits = searcher.search(query, numPages)\n",
    "            docText = \"\"\n",
    "            output = []\n",
    "            for hit in hits.scoreDocs:\n",
    "                doc_id = hit.doc\n",
    "                print doc_id, hit.toString()\n",
    "                docT = searcher.doc(hit.doc)\n",
    "                docText = docT.get(\"text\").encode(\"utf-8\")\n",
    "\n",
    "            listOfWords = docText.split(\" \")\n",
    "            numWords = len(listOfWords)\n",
    "            vector_size = 100\n",
    "            num = 0\n",
    "            docVec = np.zeros([(numWords),vector_size])\n",
    "            for eachWord in listOfWords:\n",
    "                docV = model.docvecs[doc_id]\n",
    "                docVec[num] = docV\n",
    "                num = num + 1\n",
    "\n",
    "                \n",
    "            optionList = [row[3], row[4], row[5], row[6]]\n",
    "            inputVec = np.concatenate([docVec, questionVec])\n",
    "            output = []\n",
    "            for option in optionList:\n",
    "                optionVec = model.infer_vector(tokenizeSentence(option), alpha=start_alpha, steps=infer_epoch)\n",
    "                if(answers[optionList.index(option)] == row[2]):\n",
    "                    output.append(1)\n",
    "                else:\n",
    "                    output.append(0)\n",
    "                    \n",
    "                #inputVec = merge([docVec, questionVec, optionVec], mode='concat')\n",
    "                inputVec = np.concatenate([inputVec, optionVec])\n",
    "            storeInputVecInFile[inputNum] = inputVec\n",
    "            storeOutputVecInFile[inputNum] = output\n",
    "            inputNum = inputNum + 1\n",
    "            if(inputNum % 500 == 0):\n",
    "                print inputNum, (time.time() - tic)\n",
    "# Save the input vectors and output into a file\n",
    "np.save('inputTokensModel3_100_NewCorpusWOSP_1min_2hit_4window.bin', storeInputVecInFile)\n",
    "np.save('outputTokensModel3_100_NewCorpusWOSP_1min_2hit_4window.bin', storeOutputVecInFile)\n",
    "toc = time.time()\n",
    "print toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelPath=\"/home/tarun/PE/doc2vec/model3_100_NewCorpusWOSP_1min_2hit_4window.bin\"\n",
    "model = g.Doc2Vec.load(modelPath)\n",
    "#inference hyper-parameters\n",
    "tic = time.time()\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "trainingFilePath = '/home/tarun/PE/Dataset/training_set.tsv'\n",
    "\n",
    "answers = ['A','B','C','D']\n",
    "\n",
    "with open(trainingFilePath) as f:\n",
    "    reader = csv.reader(f,delimiter = \",\")\n",
    "    data = list(reader)\n",
    "    numQuestions = len(data) - 1\n",
    "\n",
    "with open(trainingFilePath) as trainData:\n",
    "    reader = csv.reader(trainData, delimiter=\"\\t\")\n",
    "    header = 0\n",
    "    storeInputVecInFile = np.zeros([numQuestions,700])\n",
    "    storeOutputVecInFile = np.zeros([numQuestions,4])\n",
    "    inputNum = 0\n",
    "    for row in reader:\n",
    "        if (header == 0):\n",
    "            header = 1\n",
    "            continue\n",
    "        else:\n",
    "            question = row[1]\n",
    "            # Retrieving the top n (=3) documents relevant to the question\n",
    "            query = queryParser.parse(queryParser.escape(question))\n",
    "            question = tokenizeSentence(question)\n",
    "            questionVec = model.infer_vector(question, alpha=start_alpha, steps=infer_epoch)\n",
    "            numPages = 2\n",
    "            hits = searcher.search(query, numPages)\n",
    "            docVec = np.zeros(200)\n",
    "            start=0\n",
    "            output = []\n",
    "            for hit in hits.scoreDocs:\n",
    "                doc_id = hit.doc\n",
    "                print doc_id\n",
    "                docV = model.docvecs[doc_id]\n",
    "                docVec[start:start+100] = docV\n",
    "                start = start+100\n",
    "                \n",
    "            optionList = [row[3], row[4], row[5], row[6]]\n",
    "            inputVec = np.concatenate([docVec, questionVec])\n",
    "            output = []\n",
    "            for option in optionList:\n",
    "                optionVec = model.infer_vector(tokenizeSentence(option), alpha=start_alpha, steps=infer_epoch)\n",
    "                if(answers[optionList.index(option)] == row[2]):\n",
    "                    output.append(1)\n",
    "                else:\n",
    "                    output.append(0)\n",
    "                    \n",
    "                #inputVec = merge([docVec, questionVec, optionVec], mode='concat')\n",
    "                inputVec = np.concatenate([inputVec, optionVec])\n",
    "            storeInputVecInFile[inputNum] = inputVec\n",
    "            storeOutputVecInFile[inputNum] = output\n",
    "            inputNum = inputNum + 1\n",
    "            if(inputNum % 500 == 0):\n",
    "                print inputNum, (time.time() - tic)\n",
    "# Save the input vectors and output into a file\n",
    "np.save('inputTokensModel3_100_NewCorpusWOSP_1min_2hit_4window.bin', storeInputVecInFile)\n",
    "np.save('outputTokensModel3_100_NewCorpusWOSP_1min_2hit_4window.bin', storeOutputVecInFile)\n",
    "toc = time.time()\n",
    "print toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros(10)\n",
    "b = np.ones(5)\n",
    "c = np.ones(5)\n",
    "a[0:5] = b\n",
    "a[5:10] = c\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-10-27 18:58:19,015 : INFO : loading Doc2Vec object from /home/tarun/PE/doc2vec/cbse.bin\n",
      "2016-10-27 18:58:19,378 : INFO : loading docvecs recursively from /home/tarun/PE/doc2vec/cbse.bin.docvecs.* with mmap=None\n",
      "2016-10-27 18:58:19,379 : INFO : loading syn1neg from /home/tarun/PE/doc2vec/cbse.bin.syn1neg.npy with mmap=None\n",
      "2016-10-27 18:58:19,457 : INFO : setting ignored attribute syn0norm to None\n",
      "2016-10-27 18:58:19,457 : INFO : setting ignored attribute cum_table to None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "136\n",
      "136\n",
      "0.634927988052\n"
     ]
    }
   ],
   "source": [
    "modelPath=\"/home/tarun/PE/doc2vec/cbse.bin\"\n",
    "model = g.Doc2Vec.load(modelPath)\n",
    "#inference hyper-parameters\n",
    "tic = time.time()\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "trainingFilePath = '/home/tarun/PE/Dataset/passageCBSETrain.csv'\n",
    "\n",
    "answers = ['A','B','C','D']\n",
    "\n",
    "with open(trainingFilePath) as f:\n",
    "    reader = csv.reader(f,delimiter = \",\")\n",
    "    data = list(reader)\n",
    "    numQuestions = len(data) - 1\n",
    "    print numQuestions\n",
    "\n",
    "with open(trainingFilePath) as trainData:\n",
    "    reader = csv.reader(trainData, delimiter=\",\")\n",
    "    header = 0\n",
    "    storeInputVecInFile = np.zeros([(numQuestions*4),900])\n",
    "    storeOutputVecInFile = np.zeros([(numQuestions*4),1])\n",
    "    inputNum = 0\n",
    "    for row in reader:\n",
    "        if (header == 0):\n",
    "            header = 1\n",
    "            continue\n",
    "        else:\n",
    "            #print row\n",
    "            question = row[1]\n",
    "            question = tokenizeSentence(question)\n",
    "            questionVec = model.infer_vector(question, alpha=start_alpha, steps=infer_epoch)\n",
    "            #docVec = np.zeros(300)\n",
    "            passage = tokenizeSentence(row[7])\n",
    "            docVec = model.infer_vector(passage, alpha=start_alpha, steps=infer_epoch)\n",
    "            #print docVec.size\n",
    "            optionList = [row[3], row[4], row[5], row[6]]\n",
    "            for option in optionList:\n",
    "                optionVec = model.infer_vector(tokenizeSentence(option), alpha=start_alpha, steps=infer_epoch)\n",
    "                if(answers[optionList.index(option)] == row[2]):\n",
    "                    output = 1\n",
    "                else:\n",
    "                    output = 0\n",
    "                    \n",
    "                #inputVec = merge([docVec, questionVec, optionVec], mode='concat')\n",
    "                inputVec = np.concatenate([docVec, questionVec, optionVec])\n",
    "                storeInputVecInFile[inputNum] = inputVec\n",
    "                storeOutputVecInFile[inputNum] = output\n",
    "                inputNum = inputNum + 1\n",
    "                if(inputNum % 500 == 0):\n",
    "                    print inputNum, (time.time() - tic)\n",
    "# Save the input vectors and output into a file\n",
    "np.save('inputTokensPassage1.npy', storeInputVecInFile)\n",
    "np.save('outputTokensPassage1.npy', storeOutputVecInFile)\n",
    "print len(storeInputVecInFile)\n",
    "print len(storeOutputVecInFile)\n",
    "toc = time.time()\n",
    "print toc - tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test File\n",
    "## Retrieving the top n (=3) documents relevant to the question and getting question, paragraph and option vectors to create the input vector\n",
    "Test file with input vectors generated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to run this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-10-31 17:48:09,746 : INFO : loading Doc2Vec object from /home/tarun/PE/doc2vec/model3_100_NewCorpusWOSP_1min_2hit_4window.bin\n",
      "2016-10-31 17:48:10,394 : INFO : loading docvecs recursively from /home/tarun/PE/doc2vec/model3_100_NewCorpusWOSP_1min_2hit_4window.bin.docvecs.* with mmap=None\n",
      "2016-10-31 17:48:10,394 : INFO : loading syn1neg from /home/tarun/PE/doc2vec/model3_100_NewCorpusWOSP_1min_2hit_4window.bin.syn1neg.npy with mmap=None\n",
      "2016-10-31 17:48:10,522 : INFO : loading syn0 from /home/tarun/PE/doc2vec/model3_100_NewCorpusWOSP_1min_2hit_4window.bin.syn0.npy with mmap=None\n",
      "2016-10-31 17:48:10,537 : INFO : setting ignored attribute syn0norm to None\n",
      "2016-10-31 17:48:10,538 : INFO : setting ignored attribute cum_table to None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 63.940874815\n",
      "2000 124.569279909\n",
      "3000 185.44337678\n",
      "4000 245.862912893\n",
      "5000 304.016122818\n",
      "6000 363.020120859\n",
      "7000 421.743694782\n",
      "8000 482.004225969\n",
      "9000 543.718870878\n",
      "10000 603.339489937\n",
      "11000 664.023602009\n",
      "12000 724.164276838\n",
      "13000 785.66147089\n",
      "14000 843.732502937\n",
      "15000 902.652908802\n",
      "16000 965.539227962\n",
      "17000 1026.9018929\n",
      "18000 1087.19457889\n",
      "19000 1146.87776494\n",
      "20000 1206.42624378\n",
      "21000 1267.29271579\n",
      "1285.13073683\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "\n",
    "modelPath=\"/home/tarun/PE/doc2vec/model3_100_NewCorpusWOSP_1min_2hit_4window.bin\"\n",
    "model = g.Doc2Vec.load(modelPath)\n",
    "#inference hyper-parameters\n",
    "tic = time.time()\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "\n",
    "\n",
    "tic = time.time()\n",
    "import pickle\n",
    "with open('/home/tarun/PE/Dataset/final_test_set.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "numQuestions = 0\n",
    "for row in test:\n",
    "    numQuestions = numQuestions + 1\n",
    "\n",
    "storeInputVecInFileTest = np.zeros([numQuestions,700])\n",
    "    \n",
    "answers = ['A', 'B', 'C', 'D']\n",
    "checkRows = []\n",
    "inputNum = 0\n",
    "for row in test:\n",
    "    question = row[1]\n",
    "    query = queryParser.parse(queryParser.escape(question))\n",
    "    questionVec = model.infer_vector(tokenizeSentence(question), alpha=start_alpha, steps=infer_epoch)\n",
    "    numPages = 1\n",
    "    hits = searcher.search(query, numPages)\n",
    "    docVec = np.zeros(200)\n",
    "    start=0\n",
    "    output = []\n",
    "    for hit in hits.scoreDocs:\n",
    "        doc_id = hit.doc\n",
    "        docV = model.docvecs[doc_id]\n",
    "        docVec[start:start+100] = docV\n",
    "        start = start+100\n",
    "    \n",
    "    inputVec = np.concatenate([docVec, questionVec])\n",
    "    for option in [row[2], row[3], row[4], row[5]]:\n",
    "        optionVec = model.infer_vector(tokenizeSentence(option), alpha=start_alpha, steps=infer_epoch)\n",
    "        inputVec = np.concatenate([inputVec, optionVec])\n",
    "    storeInputVecInFileTest[inputNum] = inputVec\n",
    "    inputNum = inputNum + 1\n",
    "    if(inputNum % 1000 == 0):\n",
    "        print inputNum, (time.time() - tic)\n",
    "print (time.time() - tic)\n",
    "np.save('inputTokensTestModel3_100_NewCorpusWOSP_1min_2hit_4window.bin', storeInputVecInFileTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-10-27 19:02:22,493 : INFO : loading Doc2Vec object from /home/tarun/PE/doc2vec/cbse.bin\n",
      "2016-10-27 19:02:22,570 : INFO : loading docvecs recursively from /home/tarun/PE/doc2vec/cbse.bin.docvecs.* with mmap=None\n",
      "2016-10-27 19:02:22,572 : INFO : loading syn1neg from /home/tarun/PE/doc2vec/cbse.bin.syn1neg.npy with mmap=None\n",
      "2016-10-27 19:02:22,643 : INFO : setting ignored attribute syn0norm to None\n",
      "2016-10-27 19:02:22,643 : INFO : setting ignored attribute cum_table to None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "0.573496103287\n"
     ]
    }
   ],
   "source": [
    "modelPath=\"/home/tarun/PE/doc2vec/cbse.bin\"\n",
    "model = g.Doc2Vec.load(modelPath)\n",
    "#inference hyper-parameters\n",
    "tic = time.time()\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "trainingFilePath = '/home/tarun/PE/Dataset/passageCBSETest.csv'\n",
    "\n",
    "with open(trainingFilePath) as f:\n",
    "    reader = csv.reader(f,delimiter = \",\")\n",
    "    data = list(reader)\n",
    "    numQuestions = len(data) - 1\n",
    "    print numQuestions\n",
    "    \n",
    "storeInputVecInFileTest = np.zeros([(numQuestions*4),900])\n",
    "    \n",
    "answers = ['A', 'B', 'C', 'D']\n",
    "checkRows = []\n",
    "inputNum = 0\n",
    "\n",
    "\n",
    "with open(trainingFilePath) as trainData:\n",
    "    test = csv.reader(trainData, delimiter=\",\")\n",
    "    header = 0\n",
    "\n",
    "    for row in test:\n",
    "        \n",
    "        if (header == 0):\n",
    "            header = 1\n",
    "            continue\n",
    "        else:\n",
    "\n",
    "            question = row[1]\n",
    "            #print row\n",
    "            questionVec = model.infer_vector(tokenizeSentence(question), alpha=start_alpha, steps=infer_epoch)\n",
    "            passage = tokenizeSentence(row[7])\n",
    "            docVec = model.infer_vector(passage, alpha=start_alpha, steps=infer_epoch)\n",
    "            answerScores = []\n",
    "            for option in [row[3], row[4], row[5], row[6]]:\n",
    "                optionVec = model.infer_vector(tokenizeSentence(option), alpha=start_alpha, steps=infer_epoch)\n",
    "                inputVec = np.concatenate([docVec, questionVec, optionVec])\n",
    "                #print inputVec.size\n",
    "                storeInputVecInFileTest[inputNum] = inputVec\n",
    "                inputNum = inputNum + 1\n",
    "                if(inputNum % 1000 == 0):\n",
    "                    print inputNum, (time.time() - tic)\n",
    "    print (time.time() - tic)\n",
    "    np.save('inputTokensTestPassage1.npy', storeInputVecInFileTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code to be run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the stored numpy arrays (input vectors and output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelPath=\"/home/tarun/PE/doc2vec/model3_100_NewCorpusWOSP_1min_2hit_4window.bin\"\n",
    "model = g.Doc2Vec.load(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/home/tarun/PE/inputTokensModel3_100_NewCorpusWOSP_1min_2hit_4window.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-040c2185ce64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstoreInputVecInFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/tarun/PE/inputTokensModel3_100_NewCorpusWOSP_1min_2hit_4window.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstoreOutputVecInFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outputTokensModel3_100_NewCorpusWOSP_1min_2hit_4window.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstoreInputVecInFileTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inputTokensTestModel3_100_NewCorpusWOSP_1min_2hit_4window.npy.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/home/tarun/PE/inputTokensModel3_100_NewCorpusWOSP_1min_2hit_4window.npy'"
     ]
    }
   ],
   "source": [
    "storeInputVecInFile = np.load('/home/tarun/PE/inputTokensModel3_100_NewCorpusWOSP_1min_2hit_4window.npy')\n",
    "storeOutputVecInFile = np.load('outputTokensModel3_100_NewCorpusWOSP_1min_2hit_4window.npy')\n",
    "storeInputVecInFileTest = np.load('inputTokensTestModel3_100_NewCorpusWOSP_1min_2hit_4window.npy.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training the neural network for QA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the input and output vectors needed for training the QA model. The input to the neural network is a numpy array \"storeInputVecInFile\" which has input vectors (concatenation of paragraph, question and option vectors). The output for training is the numpy array \"storeOutputVecInFile\" which has 0/1 (0-wrong option) for each question-option pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "8s - loss: 8.3831 - acc: 0.2540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f63f93fb790>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = Sequential()\n",
    "\n",
    "'''\n",
    "network.add(Dense(output_dim=700, input_dim=900, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dense(500, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dense(250, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dense(50, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dense(1, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "'''\n",
    "\n",
    "network.add(Dense(output_dim=500, input_dim=700, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dropout(0.3))\n",
    "'''\n",
    "network.add(Dense(500, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dense(500, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dropout(0.2))\n",
    "network.add(Dense(500, init='uniform', activation='sigmoid'))\n",
    "network.add(Dense(500, init='uniform', activation='sigmoid'))\n",
    "network.add(Dropout(0.1))\n",
    "network.add(Dense(500, init='uniform', activation='sigmoid', activity_regularizer=activity_l2(0.01)))\n",
    "'''\n",
    "network.add(Dense(4, init='uniform', activation='softmax', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "\n",
    "\n",
    "'''\n",
    "network.add(Dense(output_dim=900, input_dim=900, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dropout(0.3))\n",
    "network.add(Dense(900, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dropout(0.3))\n",
    "network.add(Dense(900, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dropout(0.3))\n",
    "network.add(Dense(900, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dropout(0.3))\n",
    "network.add(Dense(950, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dropout(0.3))\n",
    "network.add(Dense(450, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dropout(0.3))\n",
    "network.add(Dense(1, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "'''\n",
    "network.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "#the lines above and below this comment should be run together, otherwise the parameters won't be reinitialised\n",
    "\n",
    "network.fit(storeInputVecInFile, storeOutputVecInFile, nb_epoch = 1, batch_size = 1, verbose = 2)\n",
    "#verbose = 2 is needed otherwise it gives error of I/O operations on a closed file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting output for the test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.477111101151\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "#storeInputVecInFileTest = np.load('inputTokensTestPassage1.npy')\n",
    "\n",
    "ans = network.predict(storeInputVecInFileTest, batch_size=81000, verbose=0)\n",
    "#changing the batch_size here doesnt make a difference \"apparantly\"... \n",
    "#better to have batch_size as 85191, to reduce the time\n",
    "#ans1 = network.predict_proba(storeInputVecInFileTest, batch_size=1, verbose=0)\n",
    "\n",
    "toc = time.time()\n",
    "print (toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(network, to_file='network.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21925756  0.27385992  0.27601773  0.23086479]\n",
      " [ 0.21925755  0.27385995  0.27601773  0.23086479]\n",
      " [ 0.21925753  0.27385995  0.27601773  0.23086478]\n",
      " [ 0.21925761  0.27385992  0.2760177   0.23086481]\n",
      " [ 0.21925756  0.27385992  0.27601773  0.23086479]\n",
      " [ 0.21925752  0.27385998  0.27601776  0.23086479]\n",
      " [ 0.21925753  0.27385998  0.27601779  0.23086478]\n",
      " [ 0.21925755  0.27385995  0.27601773  0.23086479]\n",
      " [ 0.21925758  0.27385992  0.27601773  0.23086481]\n",
      " [ 0.21925755  0.27385995  0.27601773  0.23086479]\n",
      " [ 0.21925744  0.27386004  0.27601781  0.23086473]\n",
      " [ 0.21925753  0.27385995  0.27601773  0.23086479]\n",
      " [ 0.21925744  0.27385998  0.27601779  0.23086472]\n",
      " [ 0.21925759  0.27385992  0.2760177   0.23086481]\n",
      " [ 0.21925761  0.27385992  0.2760177   0.23086481]\n",
      " [ 0.21925755  0.27385995  0.27601773  0.23086479]\n",
      " [ 0.21925753  0.27385995  0.27601773  0.23086479]\n",
      " [ 0.21925752  0.27385998  0.27601776  0.23086476]\n",
      " [ 0.21925755  0.27385995  0.27601773  0.23086479]\n",
      " [ 0.21925753  0.27385995  0.27601773  0.23086479]]\n"
     ]
    }
   ],
   "source": [
    "print ans[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the graphical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "0s - loss: 8331.9783 - acc: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc49c639350>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "leftNetwork = Sequential()\n",
    "leftNetwork.add(Dense(output_dim=700, input_dim=900, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "leftNetwork.add(Dense(450, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "leftNetwork.add(Dense(300, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "\n",
    "rightNetwork = Sequential()\n",
    "rightNetwork.add(Dense(output_dim=700, input_dim=900, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "\n",
    "mergedNetwork = Sequential()\n",
    "merged = Merge([leftNetwork, rightNetwork, network], mode='concat',concat_axis=1)\n",
    "mergedNetwork.add(merged)\n",
    "mergedNetwork.add(Activation('relu'))\n",
    "mergedNetwork.add(Dense(250, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "mergedNetwork.add(Dense(50, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "mergedNetwork.add(Dense(1, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "mergedNetwork.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "#the lines above and below this comment should be run together, otherwise the parameters won't be reinitialised\n",
    "\n",
    "mergedNetwork.fit([storeInputVecInFile, storeInputVecInFile, storeInputVecInFile], storeOutputVecInFile, nb_epoch = 1, batch_size = 2500, verbose = 2)\n",
    "#verbose = 2 is needed otherwise it gives error of I/O operations on a closed file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "0s - loss: 21620.5413 - acc: 0.6230\n",
      "31.876032114\n"
     ]
    }
   ],
   "source": [
    "network = Sequential()\n",
    "\n",
    "network.add(Dense(output_dim=900, input_dim=900, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dropout(0.3))\n",
    "network.add(Dense(900, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dropout(0.3))\n",
    "network.add(Dense(900, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dropout(0.3))\n",
    "network.add(Dense(900, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dropout(0.3))\n",
    "network.add(Dense(950, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dropout(0.3))\n",
    "network.add(Dense(450, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dropout(0.3))\n",
    "network.add(Dense(1, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "\n",
    "#network.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "leftNetwork = Sequential()\n",
    "leftNetwork.add(Dense(output_dim=700, input_dim=900, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "leftNetwork.add(Dense(450, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "leftNetwork.add(Dense(300, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "\n",
    "rightNetwork = Sequential()\n",
    "rightNetwork.add(Dense(output_dim=700, input_dim=900, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "\n",
    "mergedNetwork = Sequential()\n",
    "merged = Merge([leftNetwork, rightNetwork, network], mode='concat',concat_axis=1)\n",
    "mergedNetwork.add(merged)\n",
    "mergedNetwork.add(Activation('relu'))\n",
    "mergedNetwork.add(Dense(250, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "mergedNetwork.add(Dense(50, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "mergedNetwork.add(Dense(1, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "mergedNetwork.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "#the lines above and below this comment should be run together, otherwise the parameters won't be reinitialised\n",
    "\n",
    "mergedNetwork.fit([storeInputVecInFile, storeInputVecInFile, storeInputVecInFile], storeOutputVecInFile, nb_epoch = 1, batch_size = 2500, verbose = 2)\n",
    "#verbose = 2 is needed otherwise it gives error of I/O operations on a closed file\n",
    "tic = time.time()\n",
    "#storeInputVecInFileTest = np.load('inputTokensTest2.npy')\n",
    "\n",
    "ans = mergedNetwork.predict([storeInputVecInFileTest, storeInputVecInFileTest, storeInputVecInFileTest], batch_size=85191, verbose=0)\n",
    "#changing the batch_size here doesnt make a difference \"apparantly\"... \n",
    "#better to have batch_size as 85191, to reduce the time\n",
    "#ans1 = network.predict_proba(storeInputVecInFileTest, batch_size=1, verbose=0)\n",
    "\n",
    "toc = time.time()\n",
    "print (toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(mergedNetwork, to_file='network.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.43773603439\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "storeInputVecInFileTest = np.load('inputTokensTest2.npy')\n",
    "\n",
    "ans = mergedNetwork.predict([storeInputVecInFileTest, storeInputVecInFileTest], batch_size=85191, verbose=0)\n",
    "#changing the batch_size here doesnt make a difference \"apparantly\"... \n",
    "#better to have batch_size as 85191, to reduce the time\n",
    "#ans1 = network.predict_proba(storeInputVecInFileTest, batch_size=1, verbose=0)\n",
    "\n",
    "toc = time.time()\n",
    "print (toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing output to a csv file (in the format to upload on Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/home/tarun/PE/Dataset/final_test_set.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "numQuestions = 0\n",
    "for row in test:\n",
    "    numQuestions = numQuestions + 1\n",
    "\n",
    "    \n",
    "submissionFile = open(\"submit.csv\", \"w\")\n",
    "writer = csv.writer(submissionFile, delimiter=',')\n",
    "writer.writerow(['id', 'correctAnswer'])\n",
    "qIds = []\n",
    "answers = ['A', 'B', 'C', 'D']\n",
    "for row in test:\n",
    "    questionId = row[0]\n",
    "    qIds.append(questionId)\n",
    "    \n",
    "for q in range(numQuestions):\n",
    "    answerScores = []\n",
    "    opAProb = ans[q*4]\n",
    "    opBProb = ans[q*4 + 1]\n",
    "    opCProb = ans[q*4 + 2]\n",
    "    opDProb = ans[q*4 + 3]\n",
    "    answerScores.append(opAProb)\n",
    "    answerScores.append(opBProb)\n",
    "    answerScores.append(opCProb)\n",
    "    answerScores.append(opDProb)\n",
    "    writer.writerow([qIds[q] , answers[answerScores.index(np.max(answerScores))]])\n",
    "    \n",
    "submissionFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n"
     ]
    }
   ],
   "source": [
    "submissionFile = open(\"checkTest.csv\", \"w\")\n",
    "writer = csv.writer(submissionFile, delimiter=',')\n",
    "writer.writerow(['id', 'correctAnswer'])\n",
    "qIds = []\n",
    "for i in range(0,28):\n",
    "    qIds.append(i+1)\n",
    "    \n",
    "answers = ['A', 'B', 'C', 'D']\n",
    "print numQuestions, qIds\n",
    "for q in range(numQuestions):\n",
    "    answerScores = []\n",
    "    opAProb = ans[q*4]\n",
    "    opBProb = ans[q*4 + 1]\n",
    "    opCProb = ans[q*4 + 2]\n",
    "    opDProb = ans[q*4 + 3]\n",
    "    answerScores.append(opAProb)\n",
    "    answerScores.append(opBProb)\n",
    "    answerScores.append(opCProb)\n",
    "    answerScores.append(opDProb)\n",
    "    writer.writerow([qIds[q] , answers[answerScores.index(np.max(answerScores))]])\n",
    "    \n",
    "submissionFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking accuracy of doc2vec model - no need to run this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelPath=\"/home/tarun/PE/doc2vec/model2.bin\"\n",
    "modelLoad = g.Doc2Vec.load(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print modelLoad.similarity(\"heart\", \"blood\")\n",
    "print \"\\n\"\n",
    "print modelLoad.most_similar(\"blood\")\n",
    "print \"\\n\"\n",
    "print modelLoad.docvecs.most_similar(2)\n",
    "#del modelLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2, 3]) array([4, 5, 6, 7])]\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6, 7])\n",
    "l = [a,b]\n",
    "c = np.array(l)\n",
    "print c\n",
    "print type(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save('c.npy', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check = np.load('c.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2, 3]) array([4, 5, 6, 7])] <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print check, type(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[4,3,5,1], [2,6,5,1],[8,3,2,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
