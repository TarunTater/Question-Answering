{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need nltk installed\n",
    "# No need to run this part\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import string\n",
    "import re\n",
    "import operator\n",
    "import csv\n",
    "import logging\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 960M (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "# Need gensim package installed\n",
    "# No need to run this\n",
    "\n",
    "import gensim\n",
    "import gensim.models as g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need PyLucene installed\n",
    "# No need to run this\n",
    "\n",
    "import lucene\n",
    "\n",
    "from java.io import File \n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer \n",
    "from org.apache.lucene.document import Document, Field \n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig \n",
    "from org.apache.lucene.store import SimpleFSDirectory \n",
    "from org.apache.lucene.util import Version\n",
    "from org.apache.lucene.search import IndexSearcher \n",
    "from org.apache.lucene.index import IndexReader \n",
    "from org.apache.lucene.queryparser.classic import QueryParser \n",
    "\n",
    "from org.apache.lucene import document, store, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Need Keras installed\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Input, Embedding, LSTM, merge\n",
    "from keras.regularizers import l2, activity_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To get all documents in one file for input to Doc2Vec\n",
    "file already generated(allText.txt) - no need to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dont forget to remove the allText.txt file everytime you run this code.\n",
    "dirName = \"/home/tarun/PE/corpus/\"\n",
    "uniqueWords = {}\n",
    "allText = \"\"\n",
    "l = os.listdir(dirName)\n",
    "for i in range(len(l)):\n",
    "    l[i] = int(l[i][:-4])\n",
    "\n",
    "l = sorted(l)\n",
    "for i in range(len(l)):\n",
    "    l[i] = str(l[i]) + \".txt\"\n",
    "\n",
    "for fname in l:\n",
    "    with open(os.path.join(dirName, fname), 'r') as myFile:\n",
    "        fileText = myFile.read().replace('\\n', ' ') #each file's text - a paragraph like structure for us\n",
    "        for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "            fileText = fileText.replace(char, '')\n",
    "        fileText = fileText.replace(\"displaystyle\", '')\n",
    "        fileText = re.sub(\"\"\"displaystyle\"\"\", \"\", fileText, re.I|re.S)\n",
    "        fileText = re.sub(\"\"\"[^0-9A-Za-z]\"\"\", ' ', fileText)\n",
    "\n",
    "        fileText = re.sub(\"\"\"\\s+\"\"\", \" \", fileText)\n",
    "        fileText = re.sub(\"\"\"\\t+\"\"\", \" \", fileText)\n",
    "        allText = allText + fileText\n",
    "        allText = allText + \"\\n\"\n",
    "    with open(os.path.join(dirName, fname), 'w') as myFile:\n",
    "        myFile.write(fileText)\n",
    "with open(os.path.join(\"/home/tarun/PE/\", \"allTextNormal.txt\"), 'w') as myFile:\n",
    "    myFile.write(allText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of cores (to be given as paramter in the Doc2Vec training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-d7bf84e289eb>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-d7bf84e289eb>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    assert g.doc2vec.FAST_VERSION > -1,\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert g.doc2vec.FAST_VERSION > -1,\n",
    "print cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fName = \"/home/tarun/PE/allText.txt\"\n",
    "uniqueWords = {}\n",
    "with open(fName, 'r') as myFile:\n",
    "    for line in myFile:\n",
    "        #print line\n",
    "        #print \"\\n\\n\\n\"\n",
    "        sentence = line.split()\n",
    "        #print sentence\n",
    "        #print \"\\n\\n\\n\"\n",
    "        \n",
    "        for eachWord in sentence:\n",
    "            eachWord = eachWord.lower()\n",
    "            if(eachWord.isdigit() == False):\n",
    "                try:\n",
    "                    if(uniqueWords[eachWord]):\n",
    "                        uniqueWords[eachWord] = uniqueWords[eachWord] + 1\n",
    "                except:\n",
    "                    uniqueWords[eachWord] = 1\n",
    "print len(uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_x = sorted(uniqueWords.items(), key=operator.itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topUnique = dict(sorted(uniqueWords.iteritems(), key=operator.itemgetter(1), reverse=True)[:5])\n",
    "print topUnique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fName = \"/home/tarun/PE/allText.txt\"\n",
    "uniqueWords = {}\n",
    "with open(fName, 'r') as myFile:\n",
    "    for line in myFile:\n",
    "        #print line\n",
    "        #print \"\\n\\n\\n\"\n",
    "        sentence = line.split()\n",
    "        #print sentence\n",
    "        #print \"\\n\\n\\n\"\n",
    "        for eachWord in sentence:\n",
    "            eachWord = eachWord.lower()\n",
    "            if(eachWord.isdigit() == False and eachWord not in stopwords.words('english')):\n",
    "                try:\n",
    "                    if(uniqueWords[eachWord]):\n",
    "                        uniqueWords[eachWord] = uniqueWords[eachWord] + 1\n",
    "                except:\n",
    "                    uniqueWords[eachWord] = 1\n",
    "print len(uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topUnique = dict(sorted(uniqueWords.iteritems(), key=operator.itemgetter(1), reverse=True)[:5])\n",
    "print topUnique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Doc To Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391.717040062\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "#doc2vec parameters\n",
    "vector_size = 300\n",
    "window_size = 8\n",
    "min_count = 1\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100\n",
    "dm = 1 #0 = dbow; 1 = dmpv\n",
    "worker_count = 8 #number of parallel processes - number of cores\n",
    "\n",
    "#pretrained word embeddings\n",
    "#pretrained_emb = None #None if use without pretrained embeddings\n",
    "\n",
    "#input corpus\n",
    "train_corpus = \"/home/tarun/PE/allText.txt\"\n",
    "\n",
    "#output model\n",
    "saved_path = \"/home/tarun/PE/doc2vec/model3.bin\"\n",
    "\n",
    "#enable logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#train doc2vec model\n",
    "docs = g.doc2vec.TaggedLineDocument(train_corpus)\n",
    "model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1) #model3\n",
    "#model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dm_concat=1) #model2\n",
    "\n",
    "#save model\n",
    "model.save(saved_path)\n",
    "toc = time.time()\n",
    "print toc - tic\n",
    "#model2 - without dbow_words\n",
    "#model3 - with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.388097137213 1568 doc=1568 score=0.38809714 shardIndex=-1\n",
      "0.266801327467 1600 doc=1600 score=0.26680133 shardIndex=-1\n",
      "0.249090507627 1745 doc=1745 score=0.24909051 shardIndex=-1\n"
     ]
    }
   ],
   "source": [
    "# PATHS \n",
    "luceneIndexPath = '/home/tarun/PE/lucene/luceneIndexDirectory/'\n",
    "corpus = '/home/tarun/PE/corpus/'\n",
    "trainingFilePath = '/home/tarun/PE/Dataset/training_set.tsv'\n",
    "\n",
    "#parameters\n",
    "modelPath=\"/home/tarun/PE/doc2vec/model3.bin\"\n",
    "#inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "\n",
    "#load model\n",
    "#model = g.Doc2Vec.load(modelPath)\n",
    "#test_docs = [ x.strip().split() for x in codecs.open(test_docs, \"r\").readlines() ]\n",
    "#for d in test_docs:\n",
    "    #output.write( \" \".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + \"\\n\" )\n",
    "\n",
    "#output.flush()\n",
    "#output.close()\n",
    "    \n",
    "lucene.initVM()\n",
    "\n",
    "# ANALYZER\n",
    "analyzer = StandardAnalyzer(util.Version.LUCENE_CURRENT) \n",
    "\n",
    "# DIRECTORY\n",
    "directory = SimpleFSDirectory(File(luceneIndexPath))\n",
    "\n",
    "\n",
    "#dont forget to remove the luceneIndexDirectory file everytime you run this code.\n",
    "\n",
    "'''\n",
    "# INDEX WRITER\n",
    "writerConfig = IndexWriterConfig(util.Version.LUCENE_CURRENT, analyzer) \n",
    "writer = IndexWriter(directory, writerConfig)\n",
    "\n",
    "l = os.listdir(corpus)\n",
    "for i in range(len(l)):\n",
    "    l[i] = int(l[i][:-4])\n",
    "\n",
    "l = sorted(l)\n",
    "for i in range(len(l)):\n",
    "    l[i] = str(l[i]) + \".txt\"\n",
    "\n",
    "#print writer.numDocs()\n",
    "# INDEXING ALL DOCUMENTS/ARTICLES IN THE CORPUS\n",
    "for fileName in l:\n",
    "    #print fileName\n",
    "    document = Document()\n",
    "    article = os.path.join(corpus, fileName)\n",
    "    content = open(article, 'r').read()\n",
    "    document.add(Field(\"text\", content, Field.Store.YES, Field.Index.ANALYZED))\n",
    "    writer.addDocument(document)\n",
    "#print writer.numDocs()\n",
    "writer.close()\n",
    "'''\n",
    "\n",
    "# INDEX READER\n",
    "reader = IndexReader.open(directory)\n",
    "searcher = IndexSearcher(reader)\n",
    "\n",
    "# QUERYING FOR A QUESTION\n",
    "queryParser = QueryParser(util.Version.LUCENE_CURRENT, \"text\", analyzer)\n",
    "\n",
    "question = \"When athletes begin to exercise, their heart rates and respiration rates increase.  At what level of organization does the human body coordinate these functions?\"\n",
    "query = queryParser.parse(queryParser.escape(question))\n",
    "hits = searcher.search(query, 3)\n",
    "'''\n",
    "for h in hits:\n",
    "    hit = lucene.Hit.cast_(h)\n",
    "    #id, doc = hit.getId(), hit.getDocument()\n",
    "    doc_id = hit.getId()\n",
    "    print doc_id\n",
    "    model.docvecs[doc_id] \n",
    "'''\n",
    "for hit in hits.scoreDocs:\n",
    "    print hit.score, hit.doc, hit.toString()\n",
    "    #doc = searcher.doc(hit.doc)\n",
    "    #maddy = doc.get(\"text\").encode(\"utf-8\")\n",
    "    #print maddy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelPath=\"/home/tarun/PE/doc2vec/model2.bin\"\n",
    "model = g.Doc2Vec.load(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3413.68172193\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#inference hyper-parameters\n",
    "tic = time.time()\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "trainingFilePath = '/home/tarun/PE/Dataset/training_set.tsv'\n",
    "\n",
    "with open(trainingFilePath) as f:\n",
    "    reader = csv.reader(f,delimiter = \",\")\n",
    "    data = list(reader)\n",
    "    numQuestions = len(data) - 1\n",
    "\n",
    "with open(trainingFilePath) as trainData:\n",
    "    reader = csv.reader(trainData, delimiter=\"\\t\")\n",
    "    header = 0\n",
    "    storeInputVecInFile = np.zeros([(numQuestions*4),900])\n",
    "    storeOutputVecInFile = np.zeros([(numQuestions*4),1])\n",
    "    inputNum = 0\n",
    "    for row in reader:\n",
    "        if (header == 0):\n",
    "            header = 1\n",
    "            continue\n",
    "        else:\n",
    "            question = row[1]\n",
    "            query = queryParser.parse(queryParser.escape(question))\n",
    "            questionVec = model.infer_vector(question, alpha=start_alpha, steps=infer_epoch)\n",
    "            numPages = 3\n",
    "            hits = searcher.search(query, numPages)\n",
    "            docVec = np.zeros(300)\n",
    "            output = 0\n",
    "            for hit in hits.scoreDocs:\n",
    "                doc_id = hit.doc\n",
    "                docV = model.docvecs[doc_id]\n",
    "                docVec = docVec + docV\n",
    "            docVec = docVec/numPages \n",
    "            for option in [row[3], row[4], row[5], row[6]]:\n",
    "                optionVec = model.infer_vector(option, alpha=start_alpha, steps=infer_epoch)\n",
    "                if(option == row[2]):\n",
    "                    output = 1\n",
    "                else:\n",
    "                    output = 0\n",
    "                #inputVec = merge([docVec, questionVec, optionVec], mode='concat')\n",
    "                inputVec = np.concatenate([docVec, questionVec, optionVec])\n",
    "                storeInputVecInFile[inputNum] = inputVec\n",
    "                storeOutputVecInFile[inputNum] = output\n",
    "                inputNum = inputNum + 1\n",
    "\n",
    "\n",
    "np.save('input.npy', storeInputVecInFile)\n",
    "np.save('output.npy', storeOutputVecInFile)\n",
    "toc = time.time()\n",
    "print toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "storeInputVecInFile = np.load('input.npy')\n",
    "storeOutputVecInFile = np.load('output.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-2df3d5e7c570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0manswerScores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0moption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptionVec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfer_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0minputVec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocVec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestionVec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptionVec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mstoreInputVecInFileTest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputNum\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputVec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gensim/models/doc2vec.pyc\u001b[0m in \u001b[0;36minfer_vector\u001b[0;34m(self, doc_words, alpha, min_alpha, steps)\u001b[0m\n\u001b[1;32m    726\u001b[0m                 train_document_dm_concat(self, doc_words, doctag_indexes, alpha, work, neu1,\n\u001b[1;32m    727\u001b[0m                                          \u001b[0mlearn_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m                                          doctag_vectors=doctag_vectors, doctag_locks=doctag_locks)\n\u001b[0m\u001b[1;32m    729\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m                 train_document_dm(self, doc_words, doctag_indexes, alpha, work, neu1,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "import pickle\n",
    "with open('/home/tarun/PE/Dataset/final_test_set.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "numQuestions = 0\n",
    "for row in test:\n",
    "    numQuestions = numQuestions + 1\n",
    "\n",
    "storeInputVecInFileTest = np.zeros([(numQuestions*4),900])\n",
    "    \n",
    "answers = ['A', 'B', 'C', 'D']\n",
    "checkRows = []\n",
    "inputNum = 0\n",
    "for row in test:\n",
    "    question = row[1]\n",
    "    query = queryParser.parse(queryParser.escape(question))\n",
    "    questionVec = model.infer_vector(question, alpha=start_alpha, steps=infer_epoch)\n",
    "    numPages = 3\n",
    "    hits = searcher.search(query, numPages)\n",
    "    docVec = np.zeros(300)\n",
    "    for hit in hits.scoreDocs:\n",
    "        doc_id = hit.doc\n",
    "        docV = model.docvecs[doc_id]\n",
    "        docVec = docVec + docV\n",
    "    docVec = docVec/numPages \n",
    "    answerScores = []\n",
    "    for option in [row[2], row[3], row[4], row[5]]:\n",
    "        optionVec = model.infer_vector(option, alpha=start_alpha, steps=infer_epoch)\n",
    "        inputVec = np.concatenate([docVec, questionVec, optionVec])\n",
    "        storeInputVecInFileTest[inputNum] = inputVec\n",
    "        inputNum = inputNum + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "storeInputVecInFile = np.load('input.npy')\n",
    "storeOutputVecInFile = np.load('output.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s - loss: 0.1255 - acc: 1.0000     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5ae525b310>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = Sequential()\n",
    "#network.add(Dense(output_dim=300, input_dim=900, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dense(output_dim=300, input_dim=900, init='uniform', activation='sigmoid'))\n",
    "network.add(Dense(200, init='uniform', activation='sigmoid'))\n",
    "network.add(Dense(50, init='uniform', activation='sigmoid'))\n",
    "#network.add(Dense(50, init='uniform', activation='sigmoid', bias=True, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "network.add(Dense(1, init='uniform', activation='sigmoid', bias=True))\n",
    "network.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "#both these codes should be run together, otherwise the network has already been learnt.\n",
    "#tic = time.time()\n",
    "network.fit(storeInputVecInFile, storeOutputVecInFile, nb_epoch = 1, batch_size = 50, verbose = 1)\n",
    "#verbose = 2 is needed otherwise it gives error of I/O operations on a closed file\n",
    "#toc = time.time()\n",
    "#print toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.76206493378\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "storeInputVecInFileTest = np.load('inputTest.npy')\n",
    "ans = network.predict(storeInputVecInFileTest, batch_size=2500, verbose=0)\n",
    "#changing batch_size does not make a difference. Tried with 2500, 1, 50 \n",
    "#ans1 = network.predict_proba(storeInputVecInFileTest, batch_size=1, verbose=0)\n",
    "toc = time.time()\n",
    "print (toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/home/tarun/PE/Dataset/final_test_set.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "numQuestions = 0\n",
    "for row in test:\n",
    "    numQuestions = numQuestions + 1\n",
    "\n",
    "    \n",
    "submissionFile = open(\"doc2vecModelTest.csv\", \"w\")\n",
    "writer = csv.writer(submissionFile, delimiter=',')\n",
    "writer.writerow(['id', 'correctAnswer'])\n",
    "qIds = []\n",
    "answers = ['A', 'B', 'C', 'D']\n",
    "for row in test:\n",
    "    questionId = row[0]\n",
    "    qIds.append(questionId)\n",
    "    \n",
    "for q in range(numQuestions):\n",
    "    answerScores = []\n",
    "    opAProb = ans[q*4]\n",
    "    opBProb = ans[q*4 + 1]\n",
    "    opCProb = ans[q*4 + 2]\n",
    "    opDProb = ans[q*4 + 3]\n",
    "    answerScores.append(opAProb)\n",
    "    answerScores.append(opBProb)\n",
    "    answerScores.append(opCProb)\n",
    "    answerScores.append(opDProb)\n",
    "    writer.writerow([qIds[q] , answers[answerScores.index(np.max(answerScores))]])\n",
    "    \n",
    "submissionFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelPath=\"/home/tarun/PE/doc2vec/model2.bin\"\n",
    "modelLoad = g.Doc2Vec.load(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85245965204\n",
      "\n",
      "\n",
      "[(u'lungs', 0.9407699108123779), (u'bloodstream', 0.9289163947105408), (u'pumped', 0.9040083885192871), (u'air', 0.8987337946891785), (u'oxygenated', 0.8982198238372803), (u'liver', 0.8960069417953491), (u'gastroderm', 0.8852612972259521), (u'hemoglobin', 0.8786916732788086), (u'anoxic', 0.8705736398696899), (u'inside', 0.8683980107307434)]\n",
      "\n",
      "\n",
      "[(371, 0.9690296649932861), (1247, 0.9506374597549438), (366, 0.932389497756958), (128, 0.9301618337631226), (552, 0.9229782819747925), (1487, 0.9073729515075684), (46, 0.8987748622894287), (507, 0.8912057876586914), (1439, 0.8866207003593445), (912, 0.8786807060241699)]\n"
     ]
    }
   ],
   "source": [
    "#print modelLoad.most_similar('2')\n",
    "#print \"\\n\"\n",
    "print modelLoad.similarity(\"heart\", \"blood\")\n",
    "print \"\\n\"\n",
    "print modelLoad.most_similar(\"blood\")\n",
    "print \"\\n\"\n",
    "print modelLoad.docvecs.most_similar(2)\n",
    "#del modelLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = model.docvecs[2324]\n",
    "k = model.docvecs[2321] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.92552414e-310   6.92552414e-310]\n",
      " [  1.23932454e-316   1.23931980e-316]]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[[  6.92552414e-310   6.92552414e-310]\n",
      " [  1.23932454e-316   1.23931980e-316]]\n"
     ]
    }
   ],
   "source": [
    "docVecs = np.empty([2,2])\n",
    "print docVecs\n",
    "docVec = np.empty([2,2])\n",
    "print docVec\n",
    "docVecs = docVecs + docVec\n",
    "print docVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
